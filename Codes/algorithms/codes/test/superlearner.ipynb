{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a super learner model for binary classification\n",
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import sys\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Superlearner:\n",
    "\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, k):\n",
    "        self.models = list()\n",
    "        # self.meta_X, self.meta_y = list(), list()\n",
    "        # define split of data\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    # create a list of base-models\n",
    "    def get_models(self):\n",
    "        self.models.append(LogisticRegression(solver='liblinear'))\n",
    "        self.models.append(DecisionTreeClassifier())\n",
    "        self.models.append(SVC(gamma='scale', probability=True))\n",
    "        self.models.append(GaussianNB())\n",
    "        self.models.append(KNeighborsClassifier())\n",
    "        self.models.append(AdaBoostClassifier())\n",
    "        self.models.append(BaggingClassifier(n_estimators=10))\n",
    "        self.models.append(RandomForestClassifier(n_estimators=10))\n",
    "        self.models.append(ExtraTreesClassifier(n_estimators=10))\n",
    "\n",
    "    def create_dataset(self):\n",
    "        sys.path.append(os.getcwd()+'\\\\test_data\\\\deneme2.csv')  \n",
    "        path = sys.path[-1]\n",
    "        dataset = pd.read_csv(path, sep=',', header=0)\n",
    "        print(len(dataset))\n",
    "\n",
    "        # counter = -1\n",
    "        # for i in range(0, len(dataset)):\n",
    "        #     if isinstance(dataset[\"MAIL\"].iloc[i], str): \n",
    "        #     # if type(dataset[\"MAIL\"].iloc[i]) == str:\n",
    "        #          counter += 1\n",
    "        #          temp = dataset[\"MAIL\"].iloc[i]\n",
    "        #          dataset[\"MAIL\"].iloc[i] = counter\n",
    "        #     for j in range(i+1, len(dataset)):\n",
    "        #         if temp == dataset[\"MAIL\"].iloc[j]:\n",
    "        #             dataset[\"MAIL\"].iloc[j] = counter\n",
    "\n",
    "        # dataset_class_name = dataset.groupby(\"MAIL\")\n",
    "        # mail_list = dataset_class_name.groups.keys()\n",
    "        # print(mail_list)\n",
    "\n",
    "\n",
    "        # create the inputs and outputs\n",
    "        X = dataset.iloc[:, 2:-1].values\n",
    "        y = dataset.iloc[:, 0]\n",
    "        y = y.astype('string')\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    # collect out of fold predictions form k-fold cross validation\n",
    "    def get_out_of_fold_predictions(self, X, y):\n",
    "        meta_X, meta_y = list(), list()\n",
    "        # define split of data\n",
    "        kfold = KFold(n_splits = self.k, shuffle=True)\n",
    "        # enumerate splits\n",
    "        for train_ix, test_ix in kfold.split(X):\n",
    "            fold_yhats = list()\n",
    "            # get data\n",
    "            train_X, test_X = X[train_ix], X[test_ix]\n",
    "            train_y, test_y = y[train_ix], y[test_ix]\n",
    "            meta_y.extend(test_y)\n",
    "           \n",
    "            # fit and make predictions with each sub-model\n",
    "            for model in self.models:\n",
    "                model.fit(train_X, train_y)\n",
    "                yhat = model.predict_proba(test_X)\n",
    "                # store columns\n",
    "                fold_yhats.append(yhat)\n",
    "                \n",
    "            # store fold yhats as columns\n",
    "            print(len(meta_X))\n",
    "            meta_X.append(hstack(fold_yhats))\n",
    "            print(len(meta_X))\n",
    "        return vstack(meta_X), asarray(meta_y)\n",
    "\n",
    "    # fit all base models on the training dataset\n",
    "    def fit_base_models(self, X, y):\n",
    "        for model in self.models:\n",
    "            model.fit(X, y)\n",
    "\n",
    "    # fit a meta model\n",
    "    def fit_meta_model(self, X, y):\n",
    "        model = LogisticRegression(solver='liblinear')\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "    # evaluate a list of models on a dataset\n",
    "    def evaluate_models(self, X, y):\n",
    "        for model in self.models:\n",
    "            yhat = model.predict(X)\n",
    "            acc = accuracy_score(y, yhat)\n",
    "            print('%s: %.3f' % (model.__class__.__name__, acc*100))\n",
    "\n",
    "    # make predictions with stacked model\n",
    "    def super_learner_predictions(self, X, meta_model):\n",
    "        meta_X = list()\n",
    "        for model in self.models:\n",
    "            yhat = model.predict_proba(X)\n",
    "            meta_X.append(yhat)\n",
    "        meta_X = hstack(meta_X)\n",
    "        # predict\n",
    "        return meta_model.predict(meta_X)\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        # create the inputs and outputs\n",
    "        X, y = self.create_dataset() # make_blobs(n_samples=1000, centers=2, n_features=100, cluster_std=20)\n",
    "        # split\n",
    "        X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
    "        y = y.to_numpy()\n",
    "        print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
    "        # get models\n",
    "        self.get_models()\n",
    "        # get out of fold predictions\n",
    "        meta_X, meta_y = self.get_out_of_fold_predictions(X, y)\n",
    "        print('Meta ', meta_X.shape, meta_y.shape)\n",
    "        # fit base models\n",
    "        self.fit_base_models(X, y)\n",
    "        # fit the meta model\n",
    "        meta_model = self.fit_meta_model(meta_X, meta_y)\n",
    "        # evaluate base models\n",
    "        self.evaluate_models(X_val, y_val)\n",
    "        # evaluate meta model\n",
    "        yhat = self.super_learner_predictions(X_val, meta_model)\n",
    "        print('Super Learner: %.3f' % (accuracy_score(y_val, yhat) * 100))\n",
    "\n",
    "    def cm_to_df(self, cm, labels):\n",
    "        df = pd.DataFrame()\n",
    "        # rows\n",
    "        for i, row_label in enumerate(labels):\n",
    "            rowdata={}\n",
    "                \n",
    "            # columns\n",
    "            for j, col_label in enumerate(labels): \n",
    "                rowdata[col_label] = cm[i,j]\n",
    "            df = df.append(pd.DataFrame.from_dict({row_label:rowdata}, orient = 'index'))\n",
    "        return df[labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855\n",
      "Train (427, 27) (427,) Test (428, 27) (428,)\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "Meta  (427, 171) (427,)\n",
      "LogisticRegression: 57.944\n",
      "DecisionTreeClassifier: 100.000\n",
      "SVC: 54.206\n",
      "GaussianNB: 90.888\n",
      "KNeighborsClassifier: 89.953\n",
      "AdaBoostClassifier: 14.486\n",
      "BaggingClassifier: 100.000\n",
      "RandomForestClassifier: 100.000\n",
      "ExtraTreesClassifier: 100.000\n",
      "Super Learner: 100.000\n"
     ]
    }
   ],
   "source": [
    "mn = Superlearner(2)\n",
    "mn.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], [2, 2], [3, 2]])\n",
    "b = np.array([[2], [3], [4]])\n",
    "# np.vstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3],\n",
       "       [4]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda988dd22d454e497bb6f9d34084a5ab5d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
